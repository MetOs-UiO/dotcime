<?xml version="1.0"?>

<config_machines version="2.0">

  <machine MACH="fram">
    <DESC>Lenovo NeXtScale M5, 32-way nodes, dual 16-core Xeon E5-2683@2.10GHz, 64 GiB per node, os is Linux, batch system iss
 SLURM</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS>impi</MPILIBS>
    <CIME_OUTPUT_ROOT>/cluster/work/users/$USER/cesm</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/cluster/shared/noresm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$DIN_LOC_ROOT/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/cluster/work/users/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>UNSET</BASELINE_ROOT>
    <CCSM_CPRNC>UNSET</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>noresmCommunity</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>32</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>32</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="mpi-serial">
      <executable></executable>
    </mpirun>
    <mpirun mpilib="default">
      <executable>mpirun</executable>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/cluster/installations/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/cluster/installations/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="csh">/cluster/installations/lmod/lmod/init/csh</init_path>
      <init_path lang="sh">/cluster/installations/lmod/lmod/init/sh</init_path>
      <cmd_path lang="perl">/cluster/installations/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/cluster/installations/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules>
        <command name="use">/cluster/shared/noresm/eb_mods/modules/all/</command>
      	<command name="load">netCDF-Fortran/4.6.0-iimpi-2022a</command>
        <command name="load">PnetCDF/1.12.3-iimpi-2022a</command>
       <command name="load">intel/2022a</command>
       <command name="load">CMake/3.23.1-GCCcore-11.3.0</command>
       <command name="load">XML-LibXML/2.0207-GCCcore-11.3.0</command>
       <command name="load">Subversion/1.14.2-GCCcore-11.3.0</command>
     </modules>
    </module_system>
    <environment_variables>
      <env name="KMP_STACKSIZE">64M</env>
      <env name="I_MPI_EXTRA_FILESYSTEM_FORCE">lustre</env>
      <env name="I_MPI_EXTRA_FILESYSTEM">on</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

  <machine MACH="betzy">
    <DESC> BullSequana XH2000 AMD 2.2GHz, 128-way nodes, os is Linux, batch system is SLURM</DESC>
    <OS>LINUX</OS>
    <COMPILERS>intel</COMPILERS>
    <MPILIBS compiler="intel" >openmpi</MPILIBS>
    <CIME_OUTPUT_ROOT>/cluster/work/users/$USER/cesm</CIME_OUTPUT_ROOT>
    <DIN_LOC_ROOT>/cluster/shared/noresm/inputdata</DIN_LOC_ROOT>
    <DIN_LOC_ROOT_CLMFORC>$DIN_LOC_ROOT/atm/datm7</DIN_LOC_ROOT_CLMFORC>
    <DOUT_S_ROOT>/cluster/work/users/$USER/archive/$CASE</DOUT_S_ROOT>
    <BASELINE_ROOT>/cluster/shared/noresm/noresm_baselines</BASELINE_ROOT>
    <CCSM_CPRNC>/cluster/shared/noresm/tools/cprnc-iompi-2020a/cprnc</CCSM_CPRNC>
    <GMAKE_J>8</GMAKE_J>
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>
    <SUPPORTED_BY>noresmCommunity</SUPPORTED_BY>
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>
    <mpirun mpilib="mpi-serial">
      <executable></executable>
    </mpirun>
    <mpirun mpilib="default">
      <executable>srun</executable>
    </mpirun>
    <module_system type="module" allow_error="true">
      <init_path lang="perl">/cluster/installations/lmod/lmod/init/perl</init_path>
      <init_path lang="python">/cluster/installations/lmod/lmod/init/env_modules_python.py</init_path>
      <init_path lang="csh">/cluster/installations/lmod/lmod/init/csh</init_path>
      <init_path lang="sh">/cluster/installations/lmod/lmod/init/sh</init_path>
      <cmd_path lang="perl">/cluster/installations/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/cluster/installations/lmod/lmod/libexec/lmod python</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
      <modules compiler="intel">
        <command name="purge"></command>
        <command name="load">intel/2020a</command>
        <command name="load">netCDF-Fortran/4.5.2-iompi-2020a</command>
        <command name="load">iompi/2020a</command>
        <command name="load">NCO/4.9.7-iomkl-2020a</command>
        <command name="load">CMake/3.12.1</command>
        <command name="load">Python/3.9.6-GCCcore-11.2.0</command>
      </modules>
    </module_system>
    <environment_variables>
      <env name="KMP_STACKSIZE">64M</env>
      <env name="MKL_DEBUG_CPU_TYPE">5</env>
      <env name="OMPI_MCA_mpi_leave_pinned">1</env>
      <env name="OMPI_MCA_btl">self,vader</env>
      <env name="OMPI_MCA_rmaps_rank_file_physical">1</env>
      <env name="OMPI_MCA_coll_hcoll_enable">1</env>
      <env name="OMPI_MCA_coll">^fca</env>
      <env name="OMPI_MCA_coll_hcoll_priority">95</env>
      <env name="OMPI_MCA_coll_hcoll_np">8</env>
      <env name="HCOLL_MAIN_IB">mlx5_0:1</env>
      <env name="HCOLL_ENABLE_MCAST_ALL">1</env>
    </environment_variables>
    <resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>
  </machine>

</config_machines>

